{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9RdPyE4AP2wvEN4rif9Dx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZNAXNOR/AI-Blog-Posts/blob/main/AI_Blog_Posts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiNnFT3wTd2f",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# 1. Install Dependencies\n",
        "# =============================\n",
        "!pip install feedparser python-dotenv openai requests sentence-transformers scikit-learn rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 2. Imports & Environment Setup\n",
        "# =============================\n",
        "import os\n",
        "import json\n",
        "import feedparser\n",
        "import requests\n",
        "import base64\n",
        "import hashlib\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_Token')\n",
        "os.environ[\"WP_URL\"] = \"https://odlabagency.wpcomstaging.com\"\n",
        "os.environ[\"WP_USER\"] = \"odomkardalvi\"\n",
        "os.environ[\"WP_PASS\"] = userdata.get('WP_OdLabsAgency_App')\n",
        "\n",
        "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
        "WP_URL = os.environ[\"WP_URL\"]\n",
        "WP_USER = os.environ[\"WP_USER\"]\n",
        "WP_PASS = os.environ[\"WP_PASS\"]\n",
        "\n",
        "client = OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=HF_TOKEN)\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "SEMANTIC_THRESHOLD = 0.80  # Similarity cutoff for needs_review flag\n",
        "DUPLICATE_SIMILARITY = 0.90  # Duplicate blocking threshold\n",
        "TOP_K = 3"
      ],
      "metadata": {
        "id": "ZXfdh5AEY3FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 3. RSS Feed Lists\n",
        "# =============================\n",
        "GLOBAL_RSS_FEEDS = [\n",
        "    \"https://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"https://www.reutersagency.com/feed/?best-topics=technology\",\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    \"https://feeds.arstechnica.com/arstechnica/technology-lab\",\n",
        "    \"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
        "    \"https://www.cnet.com/rss/news/\",\n",
        "    \"https://www.technologyreview.com/feed/\",\n",
        "    \"https://www.wired.com/feed/category/gear/latest/rss\",\n",
        "    \"https://techcrunch.com/feed/\",\n",
        "    \"https://venturebeat.com/category/ai/feed/\",\n",
        "    \"https://www.zdnet.com/news/rss.xml\",\n",
        "    \"https://www.pcmag.com/feed\"\n",
        "]\n",
        "\n",
        "INDIA_RSS_FEEDS = [\n",
        "    \"https://www.thehindu.com/sci-tech/technology/feeder/default.rss\",\n",
        "    \"https://economictimes.indiatimes.com/tech/rssfeeds/13357270.cms\",\n",
        "    \"https://indianexpress.com/section/technology/feed/\",\n",
        "    \"https://www.livemint.com/rss/technology\",\n",
        "    \"https://timesofindia.indiatimes.com/rssfeeds/5880659.cms\"\n",
        "]\n",
        "\n",
        "Google_Trends_Google_News_RSS_FEEDS = [\n",
        "    \"https://news.google.com/rss/search?q=technology&hl=en-IN&gl=IN&ceid=IN:en\",\n",
        "    \"https://news.google.com/rss/search?q=artificial+intelligence&hl=en-US&gl=US&ceid=US:en\",\n",
        "    \"https://news.google.com/rss/search?q=machine+learning&hl=en-GB&gl=GB&ceid=GB:en\"\n",
        "]\n",
        "\n",
        "RSS_FEEDS = GLOBAL_RSS_FEEDS + INDIA_RSS_FEEDS + Google_Trends_Google_News_RSS_FEEDS\n",
        "\n",
        "def fetch_articles(feed_urls):\n",
        "    articles = []\n",
        "    for feed_url in feed_urls:\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        if feed.entries:\n",
        "            for entry in feed.entries:\n",
        "                entry['source_feed'] = feed_url\n",
        "            articles.extend(feed.entries)\n",
        "    return articles"
      ],
      "metadata": {
        "id": "pbxZZhxpNwUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 4. Semantic Clustering\n",
        "# =============================\n",
        "def cluster_articles(articles, threshold=0.75):\n",
        "    clusters, used = [], set()\n",
        "    texts = [a['title'] + \" \" + a.get('summary', '') for a in articles]\n",
        "    embeddings = embed_model.encode(texts, convert_to_numpy=True)\n",
        "    for i, art in enumerate(articles):\n",
        "        if i in used:\n",
        "            continue\n",
        "        cluster = [art]\n",
        "        used.add(i)\n",
        "        for j in range(i+1, len(articles)):\n",
        "            if j in used:\n",
        "                continue\n",
        "            sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
        "            if sim >= threshold:\n",
        "                cluster.append(articles[j])\n",
        "                used.add(j)\n",
        "        clusters.append(cluster)\n",
        "    return clusters"
      ],
      "metadata": {
        "id": "6xYen3HYY8aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 5. AI Post Generator\n",
        "# =============================\n",
        "def generate_post_with_ai(topic_articles, retries=2):\n",
        "    titles = [a['title'] for a in topic_articles]\n",
        "    summaries = [a.get('summary', '') for a in topic_articles]\n",
        "    sources = [(a['link'], a['title']) for a in topic_articles]\n",
        "\n",
        "    schema_example = {\n",
        "        \"title\": \"Example Title\",\n",
        "        \"excerpt\": \"Example excerpt...\",\n",
        "        \"tags\": [\"tag1\", \"tag2\"],\n",
        "        \"body\": \"<h3>Heading</h3><p>Paragraph...</p>\",\n",
        "        \"image_prompt\": \"A futuristic AI lab scene\"\n",
        "    }\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "    You are an expert journalist. Merge the following sources into one unified post.\n",
        "    Return ONLY valid JSON matching this schema:\n",
        "    {json.dumps(schema_example, ensure_ascii=False)}\n",
        "\n",
        "    Requirements:\n",
        "    - 900–1200 words\n",
        "    - Conversational but well-researched\n",
        "    - <h3> for headings, <h4> for subheadings, <p> for paragraphs\n",
        "    - Add a <h3>Sources</h3> section with <ol> clickable links.\n",
        "\n",
        "    Articles:\n",
        "    TITLES: {titles}\n",
        "    SUMMARIES: {summaries}\n",
        "    SOURCES: {sources}\n",
        "    \"\"\"\n",
        "\n",
        "    attempt = 0\n",
        "    while attempt <= retries:\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"openai/gpt-oss-20b\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt_template}],\n",
        "                temperature=0.7,\n",
        "                max_tokens=4000\n",
        "            )\n",
        "\n",
        "            choice = completion.choices[0]\n",
        "            content = getattr(choice.message, \"content\", None) or getattr(choice, \"text\", None)\n",
        "            if not content or not content.strip():\n",
        "                raise ValueError(\"Empty AI completion\")\n",
        "\n",
        "            match = re.search(r'\\{[\\s\\S]*\\}', content)\n",
        "            if match:\n",
        "                content = match.group(0)\n",
        "\n",
        "            parsed = json.loads(content)\n",
        "            parsed[\"body\"] = re.sub(r'\\\\n', '', parsed[\"body\"])\n",
        "            return parsed\n",
        "\n",
        "        except Exception:\n",
        "            attempt += 1\n",
        "            time.sleep(2)\n",
        "            prompt_template = \"Your last output was invalid JSON, return only valid JSON this time.\\n\" + prompt_template\n",
        "\n",
        "    raise ValueError(\"AI failed to return valid JSON\")"
      ],
      "metadata": {
        "id": "SVwJ2Z90L_7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 6. Semantic Similarity Check\n",
        "# =============================\n",
        "def compute_semantic_score(generated_html, source_articles, top_k=TOP_K):\n",
        "    def strip_html(text):\n",
        "        return re.sub(r'<[^>]*>', ' ', text)\n",
        "\n",
        "    gen_text = strip_html(generated_html)\n",
        "    src_texts = [a['title'] + \" \" + a.get('summary', '') for a in source_articles]\n",
        "\n",
        "    gen_emb = embed_model.encode([gen_text], convert_to_tensor=True, normalize_embeddings=True)\n",
        "    src_embs = embed_model.encode(src_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
        "\n",
        "    sims = util.cos_sim(gen_emb, src_embs).cpu().numpy().flatten()\n",
        "    sims_sorted = sorted(sims, reverse=True)\n",
        "    k = min(top_k, len(sims_sorted))\n",
        "    return round(float(sum(sims_sorted[:k]) / k), 4)"
      ],
      "metadata": {
        "id": "pAQ16VnVZAvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 7. Cross-Day Duplicate Detection\n",
        "# =============================\n",
        "DUPLICATE_LOG = \"published_posts.json\"\n",
        "\n",
        "def load_published_posts():\n",
        "    try:\n",
        "        with open(DUPLICATE_LOG, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return []\n",
        "\n",
        "def save_published_post(post_data):\n",
        "    posts = load_published_posts()\n",
        "    posts.append(post_data)\n",
        "    with open(DUPLICATE_LOG, \"w\") as f:\n",
        "        json.dump(posts, f)\n",
        "\n",
        "def is_duplicate(new_post):\n",
        "    past_posts = load_published_posts()\n",
        "    # Combine title, tags, and body for stronger semantic comparison\n",
        "    new_text = new_post[\"title\"] + \" \" + new_post[\"excerpt\"] + \" \" + \" \".join(new_post[\"tags\"]) + \" \" + re.sub(r'<[^>]*>', ' ', new_post[\"body\"])\n",
        "    new_emb = embed_model.encode([new_text], convert_to_tensor=True, normalize_embeddings=True)\n",
        "\n",
        "    for post in past_posts:\n",
        "        past_text = post[\"title\"] + \" \" + post.get(\"excerpt\", \"\") + \" \" + \" \".join(post[\"tags\"]) + \" \" + re.sub(r'<[^>]*>', ' ', post[\"body\"])\n",
        "        past_emb = embed_model.encode([past_text], convert_to_tensor=True, normalize_embeddings=True)\n",
        "        semantic_sim = float(util.cos_sim(new_emb, past_emb).cpu().numpy()[0][0])\n",
        "        if semantic_sim >= DUPLICATE_SIMILARITY:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "imhuwMb3ZCpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 8. WordPress Posting with Meta\n",
        "# =============================\n",
        "def ensure_tags_exist(tag_names):\n",
        "    token = base64.b64encode(f\"{WP_USER}:{WP_PASS}\".encode()).decode(\"utf-8\")\n",
        "    headers = {\"Authorization\": f\"Basic {token}\", \"Content-Type\": \"application/json\"}\n",
        "    tag_ids = []\n",
        "    for tag in tag_names:\n",
        "        r = requests.get(f\"{WP_URL}/wp-json/wp/v2/tags?search={tag}\", headers=headers)\n",
        "        if r.status_code == 200 and r.json():\n",
        "            tag_ids.append(r.json()[0]['id'])\n",
        "        else:\n",
        "            cr = requests.post(f\"{WP_URL}/wp-json/wp/v2/tags\", headers=headers, json={\"name\": tag})\n",
        "            if cr.status_code in (200, 201):\n",
        "                tag_ids.append(cr.json()['id'])\n",
        "    return tag_ids\n",
        "\n",
        "def post_to_wordpress(ai_post, semantic_score):\n",
        "    REVIEW_THRESHOLD = SEMANTIC_THRESHOLD\n",
        "    status_flag = \"approved_auto\" if semantic_score >= REVIEW_THRESHOLD else \"needs_review\"\n",
        "    token = base64.b64encode(f\"{WP_USER}:{WP_PASS}\".encode()).decode(\"utf-8\")\n",
        "    headers = {\"Authorization\": f\"Basic {token}\", \"Content-Type\": \"application/json\"}\n",
        "    tag_ids = ensure_tags_exist(ai_post[\"tags\"])\n",
        "    payload = {\n",
        "        \"title\": ai_post[\"title\"],\n",
        "        \"content\": ai_post[\"body\"],\n",
        "        \"excerpt\": ai_post[\"excerpt\"],\n",
        "        \"status\": \"draft\",\n",
        "        \"tags\": tag_ids,\n",
        "        \"meta\": {\n",
        "            \"semantic_score\": semantic_score,\n",
        "            \"status_flag\": status_flag\n",
        "        }\n",
        "    }\n",
        "    r = requests.post(f\"{WP_URL}/wp-json/wp/v2/posts\", headers=headers, json=payload)\n",
        "    if r.status_code in (200, 201):\n",
        "        print(f\"✅ Draft created: {ai_post['title']} (score={semantic_score}, flag={status_flag})\")\n",
        "        return r.json().get(\"link\")\n",
        "    print(f\"❌ Failed: {r.status_code} {r.text}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "cp5H3wBIZF76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 9. Main Flow\n",
        "# =============================\n",
        "def run_pipeline():\n",
        "    articles = fetch_articles(RSS_FEEDS)\n",
        "    clusters = cluster_articles(articles)\n",
        "    for cluster in clusters:\n",
        "        feeds_covered = len({a['source_feed'] for a in cluster})\n",
        "        if len(cluster) >= 2 and feeds_covered > 1:\n",
        "            ai_post = generate_post_with_ai(cluster)\n",
        "            if is_duplicate(ai_post):\n",
        "                print(f\"⛔ Duplicate skipped: {ai_post['title']}\")\n",
        "                continue\n",
        "            score = compute_semantic_score(ai_post[\"body\"], cluster)\n",
        "            wp_link = post_to_wordpress(ai_post, score)\n",
        "            if wp_link:\n",
        "                print(f\"Post URL: {wp_link}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "RDTcJu1ShOFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 10. Run\n",
        "# =============================\n",
        "run_pipeline()"
      ],
      "metadata": {
        "id": "owwpF1mfY0jl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}