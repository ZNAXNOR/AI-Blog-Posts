{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0lhsfESPoRZOs3Fht+0P9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZNAXNOR/AI-Blog-Posts/blob/main/AI_Blog_Posts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LiNnFT3wTd2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "858282ea-9aaf-4c27-86dc-94d9878f849e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.11/dist-packages (6.0.11)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.99.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.55.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# 1. Install Dependencies\n",
        "# =============================\n",
        "!pip install feedparser python-dotenv openai requests sentence-transformers scikit-learn rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 2. Imports & Environment Setup\n",
        "# =============================\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import base64\n",
        "import feedparser\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZXfdh5AEY3FU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Environment\n",
        "# -----------------------------\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_Token')\n",
        "os.environ[\"WP_URL\"] = \"https://odlabagency.wpcomstaging.com\"\n",
        "os.environ[\"WP_USER\"] = \"odomkardalvi\"\n",
        "os.environ[\"WP_PASS\"] = userdata.get('WP_OdLabsAgency_App')\n",
        "\n",
        "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
        "WP_URL = os.environ[\"WP_URL\"]\n",
        "WP_USER = os.environ[\"WP_USER\"]\n",
        "WP_PASS = os.environ[\"WP_PASS\"]\n",
        "\n",
        "client = OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=HF_TOKEN)\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR1J1zbuX7O9",
        "outputId": "b82c3ecc-2e25-43c8-f245-4556cc7067c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Tunables (single source of truth)\n",
        "# -----------------------------\n",
        "# Duplicate control\n",
        "DUPLICATE_SIMILARITY = 0.90        # block if >= this cosine similarity\n",
        "MEMORY_WINDOW_DAYS = 45            # keep last N days of posts in local memory\n",
        "MAX_RECENT_LOCAL = 500             # upper bound on local memory size\n",
        "\n",
        "# Coherence scoring vs sources\n",
        "SEMANTIC_COHERENCE_THRESHOLD = 0.75  # >= approved_auto, else needs_review\n",
        "TOP_K_SOURCE_MATCH = 3\n",
        "\n",
        "# WordPress fetch size (when comparing to server-side drafts as a safeguard)\n",
        "WP_FETCH_PER_PAGE = 40\n",
        "WP_FETCH_PAGES = 2                  # up to ~80 recent drafts\n",
        "\n",
        "# Retry behavior for HTTP calls\n",
        "HTTP_RETRIES = 2\n",
        "HTTP_BACKOFF = 2.0\n",
        "\n",
        "# Paths\n",
        "LOCAL_MEMORY_PATH = \"published_posts_embeddings.json\""
      ],
      "metadata": {
        "id": "mhZO3xx4YBGW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 3. RSS Feed Lists\n",
        "# =============================\n",
        "GLOBAL_RSS_FEEDS = [\n",
        "    \"https://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    \"https://feeds.arstechnica.com/arstechnica/technology-lab\",\n",
        "    \"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
        "    \"https://www.cnet.com/rss/news/\",\n",
        "    \"https://www.technologyreview.com/feed/\",\n",
        "    \"https://www.wired.com/feed/category/gear/latest/rss\",\n",
        "    \"https://techcrunch.com/feed/\",\n",
        "    \"https://venturebeat.com/category/ai/feed/\",\n",
        "    \"https://www.zdnet.com/news/rss.xml\"\n",
        "]\n",
        "\n",
        "INDIA_RSS_FEEDS = [\n",
        "    \"https://www.thehindu.com/sci-tech/technology/feeder/default.rss\",\n",
        "    \"https://economictimes.indiatimes.com/tech/rssfeeds/13357270.cms\",\n",
        "    \"https://indianexpress.com/section/technology/feed/\",\n",
        "    \"https://www.livemint.com/rss/technology\",\n",
        "    \"https://timesofindia.indiatimes.com/rssfeeds/5880659.cms\",\n",
        "]\n",
        "\n",
        "GOOGLE_NEWS_RSS = [\n",
        "    \"https://news.google.com/rss/search?q=technology&hl=en-IN&gl=IN&ceid=IN:en\",\n",
        "    \"https://news.google.com/rss/search?q=artificial+intelligence&hl=en-US&gl=US&ceid=US:en\",\n",
        "    \"https://news.google.com/rss/search?q=machine+learning&hl=en-GB&gl=GB&ceid=GB:en\",\n",
        "]\n",
        "\n",
        "RSS_FEEDS = GLOBAL_RSS_FEEDS + INDIA_RSS_FEEDS + GOOGLE_NEWS_RSS"
      ],
      "metadata": {
        "id": "pbxZZhxpNwUq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _http(headers: Dict[str, str], method: str, url: str, **kwargs) -> requests.Response:\n",
        "    for attempt in range(HTTP_RETRIES + 1):\n",
        "        try:\n",
        "            print(f\"[DEBUG] HTTP {method} {url} (attempt {attempt+1})\")\n",
        "            resp = requests.request(method, url, headers=headers, timeout=30, **kwargs)\n",
        "            print(f\"[DEBUG] HTTP status: {resp.status_code}\")\n",
        "            if resp.status_code >= 500 and attempt < HTTP_RETRIES:\n",
        "                time.sleep(HTTP_BACKOFF * (attempt + 1))\n",
        "                continue\n",
        "            return resp\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"[ERROR] HTTP request failed: {e}\")\n",
        "            if attempt >= HTTP_RETRIES:\n",
        "                raise\n",
        "            time.sleep(HTTP_BACKOFF * (attempt + 1))\n",
        "    return resp\n",
        "\n",
        "def strip_html(text: str) -> str:\n",
        "    return re.sub(r\"<[^>]*>\", \" \", text or \"\").strip()\n",
        "\n",
        "def join_tags(tags: List[str]) -> str:\n",
        "    return \" \".join(sorted([t.strip() for t in (tags or []) if t and isinstance(t, str)]))\n",
        "\n",
        "def build_unified_text(title: str, excerpt: str, tags: List[str], body_html: str) -> str:\n",
        "    return f\"{title}\\n{join_tags(tags)}\\n{excerpt}\\n{strip_html(body_html)}\".strip()"
      ],
      "metadata": {
        "id": "6xYen3HYY8aS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 5. Local Memory (embeddings + metadata)\n",
        "# =============================\n",
        "# We persist embeddings to avoid recomputation and to guard across runs.\n",
        "\n",
        "def load_memory() -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        with open(LOCAL_MEMORY_PATH, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        data = []\n",
        "    cutoff = (datetime.utcnow() - timedelta(days=MEMORY_WINDOW_DAYS)).isoformat()\n",
        "    pruned = [d for d in data if d.get(\"utc_iso\", \"\") >= cutoff]\n",
        "    return pruned[:MAX_RECENT_LOCAL]\n",
        "\n",
        "\n",
        "def save_memory(entries: List[Dict[str, Any]]):\n",
        "    with open(LOCAL_MEMORY_PATH, \"w\") as f:\n",
        "        json.dump(entries[:MAX_RECENT_LOCAL], f)\n",
        "\n",
        "\n",
        "def add_to_memory(post: Dict[str, Any]):\n",
        "    entries = load_memory()\n",
        "    entries.insert(0, post)\n",
        "    save_memory(entries)"
      ],
      "metadata": {
        "id": "SVwJ2Z90L_7o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 6. Fetch & Cluster Articles\n",
        "# =============================\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def fetch_articles(feed_urls: List[str]) -> List[Dict[str, Any]]:\n",
        "    articles = []\n",
        "    for feed_url in feed_urls:\n",
        "        print(f\"[DEBUG] Fetching feed: {feed_url}\")\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        if feed.entries:\n",
        "            print(f\"[DEBUG] {len(feed.entries)} entries fetched from {feed_url}\")\n",
        "            for entry in feed.entries:\n",
        "                entry['source_feed'] = feed_url\n",
        "                articles.append(entry)\n",
        "        else:\n",
        "            print(f\"[WARN] No entries found in {feed_url}\")\n",
        "    print(f\"[DEBUG] Total articles fetched: {len(articles)}\")\n",
        "    return articles\n",
        "\n",
        "\n",
        "def cluster_articles(articles: List[Dict[str, Any]], threshold: float = 0.75) -> List[List[Dict[str, Any]]]:\n",
        "    if not articles:\n",
        "        return []\n",
        "    texts = [(a.get('title', '') + ' ' + a.get('summary', '')) for a in articles]\n",
        "    embs = embed_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "    clusters, used = [], set()\n",
        "    for i, _ in enumerate(articles):\n",
        "        if i in used:\n",
        "            continue\n",
        "        cluster = [articles[i]]\n",
        "        used.add(i)\n",
        "        for j in range(i + 1, len(articles)):\n",
        "            if j in used:\n",
        "                continue\n",
        "            sim = float(cosine_similarity([embs[i]], [embs[j]])[0][0])\n",
        "            if sim >= threshold:\n",
        "                cluster.append(articles[j])\n",
        "                used.add(j)\n",
        "        clusters.append(cluster)\n",
        "    return clusters"
      ],
      "metadata": {
        "id": "pAQ16VnVZAvi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 7. AI Post Generator (JSON contract)\n",
        "# =============================\n",
        "def generate_post_with_ai(topic_articles: List[Dict[str, Any]], retries: int = 2) -> Dict[str, Any]:\n",
        "    titles = [a.get('title', '') for a in topic_articles]\n",
        "    summaries = [a.get('summary', '') for a in topic_articles]\n",
        "    sources = [(a.get('link', ''), a.get('title', '')) for a in topic_articles]\n",
        "\n",
        "    schema_example = {\n",
        "        \"title\": \"Example Title\",\n",
        "        \"excerpt\": \"Example excerpt...\",\n",
        "        \"tags\": [\"tag1\", \"tag2\"],\n",
        "        \"body\": \"<h3>Heading</h3><p>Paragraph...</p>\",\n",
        "        \"image_prompt\": \"A futuristic AI lab scene\"\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert journalist. Merge the following sources into one unified post.\n",
        "    Return ONLY valid JSON matching this schema:\n",
        "    {json.dumps(schema_example, ensure_ascii=False)}\n",
        "\n",
        "    Requirements:\n",
        "    - 900–1200 words\n",
        "    - Conversational but well-researched\n",
        "    - <h3> for headings, <h4> for subheadings, <p> for paragraphs\n",
        "    - Add a <h3>Sources</h3> section with <ol> clickable links.\n",
        "\n",
        "    Articles:\n",
        "    TITLES: {titles}\n",
        "    SUMMARIES: {summaries}\n",
        "    SOURCES: {sources}\n",
        "    \"\"\"\n",
        "\n",
        "    attempt = 0\n",
        "    while attempt <= retries:\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"openai/gpt-oss-20b\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.7,\n",
        "                max_tokens=4000,\n",
        "            )\n",
        "            choice = completion.choices[0]\n",
        "            content = getattr(choice, \"text\", None) or getattr(choice.message, \"content\", None)\n",
        "            if not content or not content.strip():\n",
        "                raise ValueError(\"Empty AI completion\")\n",
        "            m = re.search(r\"\\{[\\s\\S]*\\}\", content)\n",
        "            if m:\n",
        "                content = m.group(0)\n",
        "            data = json.loads(content)\n",
        "            data[\"body\"] = re.sub(r\"\\\\n\", \"\", data.get(\"body\", \"\"))\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            attempt += 1\n",
        "            time.sleep(2)\n",
        "            prompt = \"Your last output was invalid JSON, return only valid JSON.\\n\" + prompt\n",
        "    raise ValueError(\"AI failed to return valid JSON\")"
      ],
      "metadata": {
        "id": "imhuwMb3ZCpy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 8. Coherence Score vs Sources\n",
        "# =============================\n",
        "def semantic_coherence_score(generated_html: str, source_articles: List[Dict[str, Any]], top_k: int = TOP_K_SOURCE_MATCH) -> float:\n",
        "    gen_text = strip_html(generated_html)\n",
        "    src_texts = [f\"{a.get('title','')} {a.get('summary','')}\" for a in source_articles]\n",
        "    if not src_texts:\n",
        "        return 0.0\n",
        "    gen_emb = embed_model.encode([gen_text], convert_to_tensor=True, normalize_embeddings=True)\n",
        "    src_embs = embed_model.encode(src_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    sims = util.cos_sim(gen_emb, src_embs).cpu().numpy().flatten()\n",
        "    sims.sort()\n",
        "    sims = sims[::-1]\n",
        "    k = min(top_k, len(sims))\n",
        "    return round(float(sum(sims[:k]) / k), 4)"
      ],
      "metadata": {
        "id": "cp5H3wBIZF76"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 9. Duplicate Detection (Unified, Embedding-Only)\n",
        "# =============================\n",
        "# - Build a single unified text (title + tags + excerpt + body) for both new and past posts\n",
        "# - Compare cosine similarity; skip if >= DUPLICATE_SIMILARITY\n",
        "\n",
        "from numpy import ndarray\n",
        "\n",
        "def encode_paragraphs(text: str) -> List[np.ndarray]:\n",
        "    paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
        "    if not paragraphs:\n",
        "        paragraphs = [text]\n",
        "    embeddings = embed_model.encode(paragraphs, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return embeddings, paragraphs\n",
        "\n",
        "\n",
        "def is_duplicate_unified_paragraph(new_post: Dict[str, Any]) -> bool:\n",
        "    memory = load_memory()\n",
        "    if not memory:\n",
        "        return False\n",
        "\n",
        "    new_text = build_unified_text(\n",
        "        new_post.get(\"title\", \"\"),\n",
        "        new_post.get(\"excerpt\", \"\"),\n",
        "        new_post.get(\"tags\", []),\n",
        "        new_post.get(\"body\", \"\")\n",
        "    )\n",
        "    new_embs, new_paras = encode_paragraphs(new_text)\n",
        "\n",
        "    max_sim_overall = -1.0\n",
        "    for old in memory:\n",
        "        old_text = old.get(\"body_text\", \"\")\n",
        "        old_embs, old_paras = encode_paragraphs(old_text)\n",
        "        # Compute paragraph-wise similarity\n",
        "        for i, ne in enumerate(new_embs):\n",
        "            sims = [float(util.cos_sim(ne, oe)[0][0]) for oe in old_embs]\n",
        "            para_max = max(sims)\n",
        "            max_sim_overall = max(max_sim_overall, para_max)\n",
        "            if para_max >= DUPLICATE_SIMILARITY:\n",
        "                print(f\"⛔ Duplicate detected in paragraph {i} against '{old.get('title','<unknown>')}' with sim={para_max:.3f}\")\n",
        "                return True\n",
        "\n",
        "    print(f\"ℹ️ Max paragraph similarity to memory: {max_sim_overall:.3f}\")\n",
        "    return False"
      ],
      "metadata": {
        "id": "RDTcJu1ShOFQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 10. WordPress Helpers (Basic Auth, meta fields)\n",
        "# =============================\n",
        "\n",
        "def wp_headers() -> Dict[str, str]:\n",
        "    token = base64.b64encode(f\"{WP_USER}:{WP_PASS}\".encode()).decode(\"utf-8\")\n",
        "    return {\"Authorization\": f\"Basic {token}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "\n",
        "def ensure_tags_exist(tag_names: List[str]) -> List[int]:\n",
        "    headers = wp_headers()\n",
        "    ids = []\n",
        "    for tag in (tag_names or []):\n",
        "        r = _http(headers, \"GET\", f\"{WP_URL}/wp-json/wp/v2/tags?search={tag}\")\n",
        "        if r.status_code == 200 and r.json():\n",
        "            ids.append(r.json()[0][\"id\"])\n",
        "            print(f\"[DEBUG] Tags Exist\")\n",
        "            continue\n",
        "        cr = _http(headers, \"POST\", f\"{WP_URL}/wp-json/wp/v2/tags\", json={\"name\": tag})\n",
        "        if cr.status_code in (200, 201):\n",
        "            ids.append(cr.json()[\"id\"])\n",
        "    return ids\n",
        "\n",
        "\n",
        "def post_to_wordpress(ai_post: Dict[str, Any], semantic_score: float) -> str:\n",
        "    status_flag = \"approved_auto\" if semantic_score >= SEMANTIC_COHERENCE_THRESHOLD else \"needs_review\"\n",
        "    headers = wp_headers()\n",
        "    tag_ids = ensure_tags_exist(ai_post.get(\"tags\", []))\n",
        "    payload = {\n",
        "        \"title\": ai_post.get(\"title\", \"\"),\n",
        "        \"content\": ai_post.get(\"body\", \"\"),\n",
        "        \"excerpt\": ai_post.get(\"excerpt\", \"\"),\n",
        "        \"status\": \"draft\",\n",
        "        \"tags\": tag_ids,\n",
        "        \"meta\": {\n",
        "            \"semantic_score\": semantic_score,\n",
        "            \"status_flag\": status_flag,\n",
        "        },\n",
        "    }\n",
        "    r = _http(headers, \"POST\", f\"{WP_URL}/wp-json/wp/v2/posts\", json=payload)\n",
        "    if r.status_code in (200, 201):\n",
        "        print(f\"✅ Draft created: {ai_post.get('title','')} (score={semantic_score}, flag={status_flag})\")\n",
        "        link = r.json().get(\"link\", \"\")\n",
        "        # persist to local memory with embedding\n",
        "        unified_text = build_unified_text(\n",
        "            ai_post.get(\"title\",\"\"), ai_post.get(\"excerpt\",\"\"), ai_post.get(\"tags\",[]), ai_post.get(\"body\",\"\")\n",
        "        )\n",
        "        entry = {\n",
        "            \"utc_iso\": datetime.utcnow().isoformat(),\n",
        "            \"title\": ai_post.get(\"title\",\"\"),\n",
        "            \"excerpt\": ai_post.get(\"excerpt\",\"\"),\n",
        "            \"tags\": ai_post.get(\"tags\", []),\n",
        "            \"body_text\": strip_html(ai_post.get(\"body\",\"\")),\n",
        "            \"embedding\": encode_unified(unified_text).tolist(),\n",
        "            \"wp_link\": link,\n",
        "        }\n",
        "        add_to_memory(entry)\n",
        "        return link\n",
        "    print(f\"❌ WP create failed: {r.status_code} {r.text}\")\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "owwpF1mfY0jl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 11. Main Flow\n",
        "# =============================\n",
        "def run_pipeline():\n",
        "    articles = fetch_articles(RSS_FEEDS)\n",
        "    clusters = cluster_articles(articles)\n",
        "    if not clusters:\n",
        "        print(\"No clusters found.\")\n",
        "        return\n",
        "\n",
        "    # Limit number of clusters processed per run to save memory\n",
        "    for cluster_idx, cluster in enumerate(clusters[:5]):\n",
        "        feeds_covered = len({a.get('source_feed') for a in cluster})\n",
        "        if len(cluster) < 2 or feeds_covered < 2:\n",
        "            continue\n",
        "\n",
        "        print(f\"[DEBUG] Processing cluster {cluster_idx+1}/{len(clusters)} with {len(cluster)} articles\")\n",
        "        ai_post = generate_post_with_ai(cluster)\n",
        "\n",
        "        if is_duplicate_unified_paragraph(ai_post):\n",
        "            print(f\"⛔ Cluster skipped due to duplication: {ai_post.get('title','')}\")\n",
        "            continue\n",
        "\n",
        "        score = semantic_coherence_score(ai_post.get(\"body\",\"\"), cluster, TOP_K_SOURCE_MATCH)\n",
        "        link = post_to_wordpress(ai_post, score)\n",
        "        if link:\n",
        "            print(f\"Post URL: {link}\")\n",
        "            # Do not stop, process next clusters as well"
      ],
      "metadata": {
        "id": "ys32FBzhYX4R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 12. Run\n",
        "# =============================\n",
        "run_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C9iCskdYbdJ",
        "outputId": "8486d53d-9a7e-4706-e3c4-bd99d4db76dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Fetching feed: https://feeds.bbci.co.uk/news/technology/rss.xml\n",
            "[DEBUG] 64 entries fetched from https://feeds.bbci.co.uk/news/technology/rss.xml\n",
            "[DEBUG] Fetching feed: https://www.theverge.com/rss/index.xml\n",
            "[DEBUG] 10 entries fetched from https://www.theverge.com/rss/index.xml\n",
            "[DEBUG] Fetching feed: https://feeds.arstechnica.com/arstechnica/technology-lab\n",
            "[DEBUG] 20 entries fetched from https://feeds.arstechnica.com/arstechnica/technology-lab\n",
            "[DEBUG] Fetching feed: https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\n",
            "[DEBUG] 28 entries fetched from https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\n",
            "[DEBUG] Fetching feed: https://www.cnet.com/rss/news/\n",
            "[DEBUG] 25 entries fetched from https://www.cnet.com/rss/news/\n",
            "[DEBUG] Fetching feed: https://www.technologyreview.com/feed/\n",
            "[DEBUG] 10 entries fetched from https://www.technologyreview.com/feed/\n",
            "[DEBUG] Fetching feed: https://www.wired.com/feed/category/gear/latest/rss\n",
            "[DEBUG] 20 entries fetched from https://www.wired.com/feed/category/gear/latest/rss\n",
            "[DEBUG] Fetching feed: https://techcrunch.com/feed/\n",
            "[DEBUG] 20 entries fetched from https://techcrunch.com/feed/\n",
            "[DEBUG] Fetching feed: https://venturebeat.com/category/ai/feed/\n",
            "[DEBUG] 30 entries fetched from https://venturebeat.com/category/ai/feed/\n",
            "[DEBUG] Fetching feed: https://www.zdnet.com/news/rss.xml\n",
            "[DEBUG] 20 entries fetched from https://www.zdnet.com/news/rss.xml\n",
            "[DEBUG] Fetching feed: https://www.thehindu.com/sci-tech/technology/feeder/default.rss\n",
            "[DEBUG] 100 entries fetched from https://www.thehindu.com/sci-tech/technology/feeder/default.rss\n",
            "[DEBUG] Fetching feed: https://economictimes.indiatimes.com/tech/rssfeeds/13357270.cms\n",
            "[DEBUG] 50 entries fetched from https://economictimes.indiatimes.com/tech/rssfeeds/13357270.cms\n",
            "[DEBUG] Fetching feed: https://indianexpress.com/section/technology/feed/\n",
            "[DEBUG] 200 entries fetched from https://indianexpress.com/section/technology/feed/\n",
            "[DEBUG] Fetching feed: https://www.livemint.com/rss/technology\n",
            "[DEBUG] 35 entries fetched from https://www.livemint.com/rss/technology\n",
            "[DEBUG] Fetching feed: https://timesofindia.indiatimes.com/rssfeeds/5880659.cms\n",
            "[DEBUG] 20 entries fetched from https://timesofindia.indiatimes.com/rssfeeds/5880659.cms\n",
            "[DEBUG] Fetching feed: https://news.google.com/rss/search?q=technology&hl=en-IN&gl=IN&ceid=IN:en\n",
            "[DEBUG] 100 entries fetched from https://news.google.com/rss/search?q=technology&hl=en-IN&gl=IN&ceid=IN:en\n",
            "[DEBUG] Fetching feed: https://news.google.com/rss/search?q=artificial+intelligence&hl=en-US&gl=US&ceid=US:en\n",
            "[DEBUG] 102 entries fetched from https://news.google.com/rss/search?q=artificial+intelligence&hl=en-US&gl=US&ceid=US:en\n",
            "[DEBUG] Fetching feed: https://news.google.com/rss/search?q=machine+learning&hl=en-GB&gl=GB&ceid=GB:en\n",
            "[DEBUG] 100 entries fetched from https://news.google.com/rss/search?q=machine+learning&hl=en-GB&gl=GB&ceid=GB:en\n",
            "[DEBUG] Total articles fetched: 954\n",
            "[DEBUG] Processing cluster 4/849 with 2 articles\n",
            "⛔ Duplicate detected in paragraph 17 against 'Intel’s Stock Surge Amid Claims of U.S. Government Stake' with sim=1.000\n",
            "⛔ Cluster skipped due to duplication: Intel Shares Surge as Rumors Fly of a U.S. Government Stake in the Chipmaker\n"
          ]
        }
      ]
    }
  ]
}