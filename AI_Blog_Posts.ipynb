{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP0d++brFmBW8w8tNMN5sqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZNAXNOR/AI-Blog-Posts/blob/main/AI_Blog_Posts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiNnFT3wTd2f"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# 1. Install Dependencies\n",
        "# =============================\n",
        "!pip install feedparser python-dotenv openai requests sentence-transformers scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 2. Imports & Environment Setup\n",
        "# =============================\n",
        "import os\n",
        "import json\n",
        "import feedparser\n",
        "import requests\n",
        "import base64\n",
        "import hashlib\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_Token')\n",
        "os.environ[\"WP_URL\"] = \"https://odlabagency.wpcomstaging.com\"\n",
        "os.environ[\"WP_USER\"] = \"odomkardalvi\"\n",
        "os.environ[\"WP_PASS\"] = userdata.get('WP_OdLabsAgency_App')\n",
        "\n",
        "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
        "WP_URL = os.environ[\"WP_URL\"]\n",
        "WP_USER = os.environ[\"WP_USER\"]\n",
        "WP_PASS = os.environ[\"WP_PASS\"]\n",
        "\n",
        "client = OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=HF_TOKEN)\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "ZXfdh5AEY3FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 3. RSS Feed Lists\n",
        "# =============================\n",
        "GLOBAL_RSS_FEEDS = [\n",
        "    \"https://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"https://www.reutersagency.com/feed/?best-topics=technology\",\n",
        "    \"https://www.theverge.com/rss/index.xml\",\n",
        "    \"https://feeds.arstechnica.com/arstechnica/technology-lab\",\n",
        "    \"https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml\",\n",
        "    \"https://www.cnet.com/rss/news/\",\n",
        "    \"https://www.technologyreview.com/feed/\"\n",
        "]\n",
        "\n",
        "INDIA_RSS_FEEDS = [\n",
        "    \"https://www.thehindu.com/sci-tech/technology/feeder/default.rss\",\n",
        "    \"https://economictimes.indiatimes.com/tech/rssfeeds/13357270.cms\",\n",
        "    \"https://indianexpress.com/section/technology/feed/\",\n",
        "    \"https://www.livemint.com/rss/technology\",\n",
        "    \"https://timesofindia.indiatimes.com/rssfeeds/5880659.cms\"\n",
        "]\n",
        "\n",
        "RSS_FEEDS = GLOBAL_RSS_FEEDS + INDIA_RSS_FEEDS\n",
        "\n",
        "def fetch_articles(feed_urls):\n",
        "    articles = []\n",
        "    for feed_url in feed_urls:\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        if feed.entries:\n",
        "            for entry in feed.entries:\n",
        "                entry['source_feed'] = feed_url\n",
        "            articles.extend(feed.entries)\n",
        "    return articles"
      ],
      "metadata": {
        "id": "6xYen3HYY8aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 4. Semantic Clustering\n",
        "# =============================\n",
        "def cluster_articles(articles, threshold=0.75):\n",
        "    clusters, used = [], set()\n",
        "    texts = [a['title'] + \" \" + a.get('summary', '') for a in articles]\n",
        "    embeddings = embed_model.encode(texts, convert_to_numpy=True)\n",
        "    for i, art in enumerate(articles):\n",
        "        if i in used:\n",
        "            continue\n",
        "        cluster = [art]\n",
        "        used.add(i)\n",
        "        for j in range(i+1, len(articles)):\n",
        "            if j in used:\n",
        "                continue\n",
        "            sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
        "            if sim >= threshold:\n",
        "                cluster.append(articles[j])\n",
        "                used.add(j)\n",
        "        clusters.append(cluster)\n",
        "    return clusters"
      ],
      "metadata": {
        "id": "SVwJ2Z90L_7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 5. AI Post Generator with JSON Reliability and Sources Section\n",
        "# =============================\n",
        "def generate_post_with_ai(topic_articles, retries=2):\n",
        "    titles = [a['title'] for a in topic_articles]\n",
        "    summaries = [a.get('summary', '') for a in topic_articles]\n",
        "    sources = [(a['link'], a['title']) for a in topic_articles]\n",
        "\n",
        "    schema_example = {\n",
        "        \"title\": \"Example Title\",\n",
        "        \"excerpt\": \"Example excerpt...\",\n",
        "        \"tags\": [\"tag1\", \"tag2\"],\n",
        "        \"body\": \"<h3>Heading</h3><p>Paragraph...</p>\",\n",
        "        \"image_prompt\": \"A futuristic AI lab scene\"\n",
        "    }\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "    You are an expert journalist. Merge the following sources into one unified post.\n",
        "    Return ONLY valid JSON matching this schema:\n",
        "    {json.dumps(schema_example, ensure_ascii=False)}\n",
        "\n",
        "    Requirements:\n",
        "    - 900–1200 words, 3–5 min read\n",
        "    - Conversational but well-researched\n",
        "    - <h3> for headings, <h4> for subheadings, <p> for paragraphs\n",
        "    - Do not use \\n for line breaks; use proper HTML tags only.\n",
        "    - Do not use \\\\n for line breaks; use proper HTML tags only.\n",
        "    - Add a <h3>Sources</h3> section at the bottom with an ordered <ol> list of clickable <a> links to each source.\n",
        "\n",
        "    Articles:\n",
        "    TITLES: {titles}\n",
        "    SUMMARIES: {summaries}\n",
        "    SOURCES: {sources}\n",
        "    \"\"\"\n",
        "\n",
        "    attempt = 0\n",
        "    while attempt <= retries:\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"openai/gpt-oss-20b\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt_template}],\n",
        "                temperature=0.7,\n",
        "                max_tokens=4000\n",
        "            )\n",
        "\n",
        "            # Support both .message.content and .messages[0].content style returns\n",
        "            choice = completion.choices[0]\n",
        "            content = None\n",
        "\n",
        "            if hasattr(choice, \"message\") and hasattr(choice.message, \"content\"):\n",
        "                content = choice.message.content\n",
        "            elif hasattr(choice, \"text\"):\n",
        "                content = choice.text\n",
        "\n",
        "            if not content or not content.strip():\n",
        "                raise ValueError(f\"Empty AI completion received. Raw: {completion}\")\n",
        "\n",
        "\n",
        "            match = re.search(r'\\{[\\s\\S]*\\}', content)\n",
        "            if match:\n",
        "                content = match.group(0)\n",
        "\n",
        "            parsed = json.loads(content)\n",
        "            parsed[\"body\"] = re.sub(r'\\\\n', '', parsed[\"body\"])\n",
        "            return parsed\n",
        "\n",
        "        except Exception as e:\n",
        "            attempt += 1\n",
        "            print(f\"Retry {attempt}/{retries} - {type(e).__name__}: {e}\")\n",
        "            time.sleep(2)\n",
        "            prompt_template = \"Your last output was invalid JSON or empty, return only valid JSON this time.\\n\" + prompt_template\n",
        "\n",
        "    raise ValueError(f\"AI failed to return valid JSON after {retries+1} attempts\")"
      ],
      "metadata": {
        "id": "pAQ16VnVZAvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 6. WordPress Posting\n",
        "# =============================\n",
        "def ensure_tags_exist(tag_names):\n",
        "    token = base64.b64encode(f\"{WP_USER}:{WP_PASS}\".encode()).decode(\"utf-8\")\n",
        "    headers = {\"Authorization\": f\"Basic {token}\", \"Content-Type\": \"application/json\"}\n",
        "    tag_ids = []\n",
        "    for tag in tag_names:\n",
        "        r = requests.get(f\"{WP_URL}/wp-json/wp/v2/tags?search={tag}\", headers=headers)\n",
        "        if r.status_code == 200 and r.json():\n",
        "            tag_ids.append(r.json()[0]['id'])\n",
        "        else:\n",
        "            cr = requests.post(f\"{WP_URL}/wp-json/wp/v2/tags\", headers=headers, json={\"name\": tag})\n",
        "            if cr.status_code in (200, 201):\n",
        "                tag_ids.append(cr.json()['id'])\n",
        "    return tag_ids\n",
        "\n",
        "def post_to_wordpress(ai_post):\n",
        "    token = base64.b64encode(f\"{WP_USER}:{WP_PASS}\".encode()).decode(\"utf-8\")\n",
        "    headers = {\"Authorization\": f\"Basic {token}\", \"Content-Type\": \"application/json\"}\n",
        "    tag_ids = ensure_tags_exist(ai_post[\"tags\"])\n",
        "    r = requests.post(f\"{WP_URL}/wp-json/wp/v2/posts\", headers=headers, json={\n",
        "        \"title\": ai_post[\"title\"],\n",
        "        \"content\": ai_post[\"body\"],\n",
        "        \"excerpt\": ai_post[\"excerpt\"],\n",
        "        \"status\": \"draft\",\n",
        "        \"tags\": tag_ids\n",
        "    })\n",
        "    if r.status_code in (200, 201):\n",
        "        print(f\"✅ Draft created: {ai_post['title']}\")\n",
        "        return r.json().get(\"link\")\n",
        "    print(f\"❌ Failed: {r.status_code} {r.text}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "imhuwMb3ZCpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 7. Duplicate Detection (Local + WordPress)\n",
        "# =============================\n",
        "def load_published():\n",
        "    try:\n",
        "        with open(\"published_topics.json\", \"r\") as f:\n",
        "            return set(json.load(f))\n",
        "    except FileNotFoundError:\n",
        "        return set()\n",
        "\n",
        "def save_published(published_set):\n",
        "    with open(\"published_topics.json\", \"w\") as f:\n",
        "        json.dump(list(published_set), f)\n",
        "\n",
        "def topic_signature(title):\n",
        "    return hashlib.sha256(title.lower().encode()).hexdigest()"
      ],
      "metadata": {
        "id": "cp5H3wBIZF76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 8. Main Flow\n",
        "# =============================\n",
        "def run_pipeline():\n",
        "    published_topics = load_published()\n",
        "    articles = fetch_articles(RSS_FEEDS)\n",
        "    clusters = cluster_articles(articles)\n",
        "    for cluster in clusters:\n",
        "        feeds_covered = len({a['source_feed'] for a in cluster})\n",
        "        if len(cluster) >= 2 and feeds_covered > 1:\n",
        "            ai_post = generate_post_with_ai(cluster)\n",
        "            sig = topic_signature(ai_post[\"title\"])\n",
        "            if sig not in published_topics:\n",
        "                wp_link = post_to_wordpress(ai_post)\n",
        "                if wp_link:\n",
        "                    published_topics.add(sig)\n",
        "                    save_published(published_topics)\n",
        "            break"
      ],
      "metadata": {
        "id": "RDTcJu1ShOFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# 9. Run\n",
        "# =============================\n",
        "run_pipeline()"
      ],
      "metadata": {
        "id": "owwpF1mfY0jl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}